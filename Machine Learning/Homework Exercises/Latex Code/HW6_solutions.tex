\documentclass[12pt]{book}

%These tell TeX which packages to use.
\usepackage{array,epsfig}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsxtra}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{color}
\usepackage{enumitem}
\usepackage{bm}
\usepackage{commath}
\usepackage{bbm}

\newcommand{\bb}[1]{\mathbb{#1}}
\newcommand{\R}{\bb{R}}

%Pagination stuff.
\setlength{\topmargin}{-.3 in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\textheight}{9.in}
\setlength{\textwidth}{6.5in}
\pagestyle{empty}

\setlength\parindent{0pt}



\begin{document}

\begin{center}
{\Large Machine Learning and Computational Statistics}\\
\large\textbf{Theodoros Georgiopoulos}\\ %You should put your name here
\Large Homework 6 %You should write the date here.
\end{center}

\vspace{0.2 cm}
\begin{center}
	\subsection*{Exercises for Unit 6: Parametric pdf estimation}
\end{center}
\vspace{0.2 cm}

{\underline{\large Solution for exercise 1}}
\vspace{0.1 cm}

\noindent

We have the problem
\begin{align*}
\min J(\mu) = \sum_{n=1}^{N}(x_n-\mu)^2 \\
\text{subject to } (\mu - \mu_0)^2 \leq \rho.
\end{align*}
We define the Lagrangian function
\begin{align*}
L(\mu) = \sum_{n=1}^{N}(x_n-\mu)^2 + \lambda((\mu - \mu_0)^2 - \rho).
\end{align*}
The derivative with respect to $\mu$ is
\begin{align*}
\frac{\partial{L(\mu)}}{\partial{\mu}} &= -2\sum_{n=1}^{N}(x_n-\mu) + 2\lambda\mu - 2\lambda \mu_0 \\
&= -2\sum_{n=1}^{N}x_n + 2N\mu + 2\lambda\mu - 2\lambda \mu_0.
\end{align*}
We set the derivative equal to zero and we solve the equation for finding $\mu$.
\begin{align*}
\frac{\partial{L(\mu)}}{\partial{\mu}} = 0 \iff \mu = \frac{\lambda\mu_0 + \sum_{n=1}^{N}x_n}{N + \lambda}
\end{align*}

\vspace{0.3 cm}
{\underline{\large Solution for exercise 2}}
\vspace{0.3 cm}

The data are modeled by a pdf of the form
\begin{align*}
p(\pmb{x}) = \sum_{j=1}^{m} P_j p(\pmb{x}|j), \ \ \  \sum_{j=1}^{m}P_j = 1, \ \ \ \int_{-\infty}^{+\infty}p(\pmb{x}|j)  = 1
\end{align*}
The parameter updating part of the Expectation Maximixation (EM) algorithm is
\begin{align*}
\arg \ max_{P_1,P_2,\dots,P_M} \sum_{i=1}^{N} \sum_{j=1}^{m}P(j|\pmb{x_i})\ln P_j \\
\text{subject to} \sum_{j=1}^{m}P_j = 1. \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
\end{align*}
\begin{enumerate}[label=(\alph*)]
	\item We define the Lagrangian function
	\begin{align*}
	L(P_1,P_2,\dots,P_m) = \sum_{i=1}^{N} \sum_{j=1}^{m}P(j|\pmb{x_i})\ln P_j + \lambda(\sum_{j=1}^{m}P_j - 1).
	\end{align*}
	\item We set the derivative of the Lagrangian function with respect to $P_j$ equal to zero and we solve for $P_j$.
	\begin{align*}
	\frac{\partial L(P_1,P_2,\dots,P_m)}{\partial P_j} = 0 \iff \sum_{i=1}^{N} P(j|\pmb{x_i})\frac{1}{P_j} + \lambda = 0
	\end{align*}
	So,
	\begin{align}
	P_j = - \frac{\sum_{i=1}^{N} P(j|\pmb{x_i})}{\lambda}
	\end{align}
	\item Also we have
	\begin{align}
	\sum_{j=1}^{m}P_j = 1.
	\end{align}
	From (1),(2)
	\begin{align*}
	\sum_{j=1}^{m} \frac{\sum_{i=1}^{N} P(j|\pmb{x_i})}{\lambda} = -1
	\end{align*}
	and
	\begin{align}
	\lambda = - \sum_{j=1}^{m} \sum_{i=1}^{N} P(j|\pmb{x_i}) = -N.
	\end{align}
	\item 
	Finally, from (1),(3)
	\begin{align*}
	P_j = \frac{\sum_{i=1}^{N} P(j|\pmb{x_i})}{N}.
	\end{align*}
\end{enumerate}




\end{document}


